{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Agend:**\n",
    "\n",
    "1. initialize Model Parameter/初始化模型参数\n",
    "\n",
    "2. Forword /前向传播\n",
    "\n",
    "    2.1. Lexical Features /语法特征\n",
    "    \n",
    "    2.2. Sentence Features /句子特征\n",
    "    \n",
    "    2.3. Fully-connection Softmax 全连接分类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, word_vec, class_num, config):\n",
    "        super().__init__()\n",
    "        self.word_vec = word_vec\n",
    "        self.class_num = class_num\n",
    "\n",
    "        # hyper parameters and others\n",
    "        self.max_len = config.max_len\n",
    "        self.word_dim = config.word_dim\n",
    "        self.pos_dim = config.pos_dim\n",
    "        self.pos_dis = config.pos_dis\n",
    "\n",
    "        self.dropout_value = config.dropout\n",
    "        self.filter_num = config.filter_num\n",
    "        self.window = config.window\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.dim = self.word_dim + 2 * self.pos_dim\n",
    "\n",
    "        # load the pre-train model for word features\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(\n",
    "            embeddings=self.word_vec,\n",
    "            freeze=False,\n",
    "        )\n",
    "        # embedding for features for position distance relative to entity 1\n",
    "        self.pos1_embedding = nn.Embedding(\n",
    "            num_embeddings=2 * self.pos_dis + 3,\n",
    "            embedding_dim=self.pos_dim\n",
    "        )\n",
    "         # embedding for features for position distance  relative to entity 2\n",
    "        self.pos2_embedding = nn.Embedding(\n",
    "            num_embeddings=2 * self.pos_dis + 3,\n",
    "            embedding_dim=self.pos_dim\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=self.filter_num,\n",
    "            kernel_size=(self.window, self.dim),\n",
    "            stride=(1, 1),\n",
    "            bias=False,\n",
    "            padding=(1, 0),  # same padding\n",
    "            padding_mode='zeros'\n",
    "        )\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d((self.max_len, 1))\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout_value)\n",
    "        \n",
    "        self.linear = nn.Linear(\n",
    "            in_features=self.filter_num,\n",
    "            out_features=self.hidden_size,\n",
    "            bias=False\n",
    "        )\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=self.hidden_size+6*self.word_dim,\n",
    "            out_features=self.class_num,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # initialize weight\n",
    "        init.xavier_normal_(self.pos1_embedding.weight)\n",
    "        init.xavier_normal_(self.pos2_embedding.weight)\n",
    "        init.xavier_normal_(self.conv.weight)\n",
    "        #init.constant_(self.conv.bias, 0.)\n",
    "        init.xavier_normal_(self.linear.weight)\n",
    "        #init.constant_(self.linear.bias, 0.)\n",
    "        init.xavier_normal_(self.dense.weight)\n",
    "        #init.constant_(self.dense.bias, 0.)\n",
    "\n",
    "    def encoder_layer(self, token, pos1, pos2):\n",
    "        word_emb = self.word_embedding(token)  # B*L*word_dim\n",
    "        pos1_emb = self.pos1_embedding(pos1)  # B*L*pos_dim\n",
    "        pos2_emb = self.pos2_embedding(pos2)  # B*L*pos_dim\n",
    "        emb = torch.cat(tensors=[word_emb, pos1_emb, pos2_emb], dim=-1)\n",
    "        return emb  # B*L*D, D=word_dim+2*pos_dim\n",
    "\n",
    "    def conv_layer(self, emb, mask):\n",
    "        emb = emb.unsqueeze(dim=1)  # B*1*L*D\n",
    "        conv = self.conv(emb)  # B*C*L*1\n",
    "\n",
    "        # mask, remove the effect of 'PAD'\n",
    "        conv = conv.view(-1, self.filter_num, self.max_len)  # B*C*L\n",
    "        mask = mask.unsqueeze(dim=1)  # B*1*L\n",
    "        mask = mask.expand(-1, self.filter_num, -1)  # B*C*L\n",
    "        conv = conv.masked_fill_(mask.eq(0), float('-inf'))  # B*C*L\n",
    "        conv = conv.unsqueeze(dim=-1)  # B*C*L*1\n",
    "        return conv\n",
    "\n",
    "    def single_maxpool_layer(self, conv):\n",
    "        pool = self.maxpool(conv)  # B*C*1*1\n",
    "        pool = pool.view(-1, self.filter_num)  # B*C\n",
    "        return pool\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        token = data[0][:, 0, :].view(-1, self.max_len)\n",
    "        pos1 = data[0][:, 1, :].view(-1, self.max_len)\n",
    "        pos2 = data[0][:, 2, :].view(-1, self.max_len)\n",
    "        mask = data[0][:, 3, :].view(-1, self.max_len)\n",
    "        lexical = data[1].view(-1, 6)\n",
    "        \n",
    "        lexical_emb = self.word_embedding(lexical)\n",
    "        lexical_emb = lexical_emb.view(-1, self.word_dim * 6)\n",
    "        \n",
    "        emb = self.encoder_layer(token, pos1, pos2)\n",
    "        emb = self.dropout(emb)\n",
    "        conv = self.conv_layer(emb, mask)\n",
    "        pool = self.single_maxpool_layer(conv)\n",
    "        \n",
    "        sentence_feature = self.linear(pool)\n",
    "        sentence_feature = self.tanh(sentence_feature)\n",
    "        sentence_feature = self.dropout(sentence_feature)\n",
    "        \n",
    "        features = torch.cat((lexical_emb, sentence_feature), 1)\n",
    "        logits = self.dense(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameter tuple\n",
    "from collections import namedtuple\n",
    "conf = namedtuple('conf',['max_len','word_dim','pos_dim',\\\n",
    "                          'pos_dis','dropout','filter_num','window','hidden_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dim  = 300 # word dimenstion \n",
    "max_len = 64 # max_len for each setence input\n",
    "pos_dim = 300 # dimentison for postion embeddinhg\n",
    "pos_dis = 32 # position distance for relative position encoding\n",
    "dropout = 0.5\n",
    "filter_num = 16 # the number for feature map\n",
    "window = 3 # kernal size of window\n",
    "hidden_size = 100\n",
    "num_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec=torch.rand(num_words,word_dim)\n",
    "class_num=19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=conf(max_len,word_dim,pos_dim,pos_dis,dropout,filter_num,window,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CNN(word_vec, class_num, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (word_embedding): Embedding(10, 300)\n",
      "  (pos1_embedding): Embedding(67, 300)\n",
      "  (pos2_embedding): Embedding(67, 300)\n",
      "  (conv): Conv2d(1, 16, kernel_size=(3, 900), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "  (maxpool): MaxPool2d(kernel_size=(64, 1), stride=(64, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (tanh): Tanh()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=16, out_features=100, bias=False)\n",
      "  (dense): Linear(in_features=1900, out_features=19, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Imgs/wordfeatures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "position features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Imgs/positionfeatures.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake data\n",
    "import numpy as np\n",
    "# two sample \n",
    "lexical=torch.from_numpy(np.random.randint(10,size=(2, 6))).long()\n",
    "# lexical input  = (left of entity1 ,entity1,right of entity 1 )\n",
    "sentence=torch.from_numpy(np.random.randint(10,size=(2,4,max_len))).long()\n",
    "# [words id, pos enc relative to  entity 1, pos enc relative to entity 2, mask] with max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (sentence,lexical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = data[0][:, 0, :].view(-1, max_len)\n",
    "pos1 = data[0][:, 1, :].view(-1, max_len)\n",
    "pos2 = data[0][:, 2, :].view(-1, max_len)\n",
    "mask = data[0][:, 3, :].view(-1, max_len)\n",
    "lexical = data[1].view(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = nn.Embedding.from_pretrained(\n",
    "    embeddings=word_vec,\n",
    "    freeze=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 5, 3, 1, 4, 9],\n",
       "        [2, 4, 2, 9, 2, 6]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_emb = word_embedding(lexical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 300])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_emb = lexical_emb.view(-1,word_dim * 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding for features for position distance relative to entity 1\n",
    "pos1_embedding = nn.Embedding(\n",
    "    num_embeddings=2 * pos_dis + 3,\n",
    "    embedding_dim=pos_dim\n",
    ")\n",
    " # embedding for features for position distance  relative to entity 2\n",
    "pos2_embedding = nn.Embedding(\n",
    "    num_embeddings=2 * pos_dis + 3,\n",
    "    embedding_dim=pos_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(token, pos1, pos2):\n",
    "    word_emb = word_embedding(token)  # B*L*word_dim\n",
    "    pos1_emb = pos1_embedding(pos1)  # B*L*pos_dim\n",
    "    pos2_emb = pos2_embedding(pos2)  # B*L*pos_dim\n",
    "    emb = torch.cat(tensors=[word_emb, pos1_emb, pos2_emb], dim=-1)\n",
    "    return emb  # B*L*D, D=word_dim+2*pos_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb= encoder_layer(token, pos1, pos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 900])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape # (batch_size,max_len,[word_emd_feature + pos1_feaures + pos2_features ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb = self.dropout(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = word_dim + 2 * pos_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Imgs/forward_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=filter_num,\n",
    "    kernel_size=(window, dim), # (3,900) [three word for convolution]\n",
    "    stride=(1, 1),\n",
    "    bias=False,\n",
    "    padding=(1, 0),  # same padding -> add zero to the top\n",
    "    padding_mode='zeros'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_unsqueeze = emb.unsqueeze(dim=1) # expand_dims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64, 900])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_unsqueeze.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = conv(emb_unsqueeze)  # B*C*L*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64, 1])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output= conv_output.view(-1,filter_num,max_len)  # B*C*L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1], [2], [3]])\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask.unsqueeze(dim=1)  # B*1*L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask.expand(-1,filter_num, -1)  # B*C*L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4, 4, 6,  ..., 2, 4, 0],\n",
       "         [4, 4, 6,  ..., 2, 4, 0],\n",
       "         [4, 4, 6,  ..., 2, 4, 0],\n",
       "         ...,\n",
       "         [4, 4, 6,  ..., 2, 4, 0],\n",
       "         [4, 4, 6,  ..., 2, 4, 0],\n",
       "         [4, 4, 6,  ..., 2, 4, 0]],\n",
       "\n",
       "        [[4, 4, 7,  ..., 2, 0, 4],\n",
       "         [4, 4, 7,  ..., 2, 0, 4],\n",
       "         [4, 4, 7,  ..., 2, 0, 4],\n",
       "         ...,\n",
       "         [4, 4, 7,  ..., 2, 0, 4],\n",
       "         [4, 4, 7,  ..., 2, 0, 4],\n",
       "         [4, 4, 7,  ..., 2, 0, 4]]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = conv_output.masked_fill_(mask.eq(0), float('-inf'))  # B*C*L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = conv_output.unsqueeze(dim=-1)  # B*C*L*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64, 1])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def conv_layer(self, emb, mask):\n",
    "    emb = emb.unsqueeze(dim=1)  # B*1*L*D\n",
    "    conv = self.conv(emb)  # B*C*L*1\n",
    "\n",
    "    # mask, remove the effect of 'PAD'\n",
    "    conv = conv.view(-1, self.filter_num, self.max_len)  # B*C*L\n",
    "    mask = mask.unsqueeze(dim=1)  # B*1*L\n",
    "    mask = mask.expand(-1, self.filter_num, -1)  # B*C*L\n",
    "    conv = conv.masked_fill_(mask.eq(0), float('-inf'))  # B*C*L\n",
    "    conv = conv.unsqueeze(dim=-1)  # B*C*L*1\n",
    "    return conv\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Imgs/forward_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool = nn.MaxPool2d((max_len, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_maxpool_layer(conv):\n",
    "    pool = maxpool(conv)  # B*C*1*1\n",
    "    pool = pool.view(-1, filter_num)  # B*C\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = single_maxpool_layer(conv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(\n",
    "    in_features=filter_num,\n",
    "    out_features=hidden_size,\n",
    "    bias=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_feature = linear(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "# sentence_feature = self.tanh(sentence_feature)\n",
    "# sentence_feature = self.dropout(sentence_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1800])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.cat((lexical_emb, sentence_feature), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = nn.Linear(\n",
    "    in_features=hidden_size+6*word_dim,\n",
    "    out_features=class_num,\n",
    "    bias=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = dense(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 19])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1016,  0.3743, -0.4437, -0.4014, -0.2638, -0.3386,  0.7015, -0.2964,\n",
       "         -0.1179,  0.0546,  0.0197, -0.0047,  0.1042, -0.1399, -0.6228,  0.0522,\n",
       "         -0.1237,  0.1739, -0.0660],\n",
       "        [-0.0889,  0.2659, -0.0040, -0.4476, -0.2312, -0.3862,  0.9071, -0.3338,\n",
       "         -0.2121,  0.0182, -0.2821,  0.2306, -0.0267,  0.1076, -0.4381,  0.0658,\n",
       "          0.0440,  0.3762, -0.1850]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
