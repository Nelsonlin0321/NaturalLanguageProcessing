{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Agend：**\n",
    "\n",
    "1. Covert row data to be JSON/将原始数据转化为json格式\n",
    "\n",
    "2. Load the pretrained word vectors/预加载词向量\n",
    "\n",
    "3. Replation Encoding/关系编码\n",
    "\n",
    "4. Training Sample Generation 样本生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTsg5j62xfnk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Covert row data to be JSON\n",
    "input /输入：\n",
    "```\n",
    "8001\t\"The most common <e1>audits</e1> were about <e2>waste</e2> and recycling.\"\n",
    "Message-Topic(e1,e2)\n",
    "Comment: Assuming an audit = an audit document.\n",
    "```\n",
    "output /输出:\n",
    "```\n",
    "{\"id\": \"8001\", \"relation\": \"Message-Topic(e1,e2)\", \"head\": \"audits\", \"tail\": \"waste\", \"subj_start\": 3, \"subj_end\": 3, \"obj_start\": 6, \"obj_end\": 6, \"sentence\": [\"The\", \"most\", \"common\", \"audits\", \"were\", \"about\", \"waste\", \"and\", \"recycling\", \".\"], \"comment\": \" Assuming an audit = an audit document.\"}\n",
    "```\n",
    "\n",
    "　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The most common <e1>audits</e1> were about <e2>waste</e2> and recycling.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = re.findall(r'<e1>(.*)</e1>', sentence)[0]\n",
    "e2 = re.findall(r'<e2>(.*)</e2>', sentence)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentence.replace('<e1>' + e1 + '</e1>', ' <e1> ' + e1 + ' </e1> ', 1)\n",
    "sentence = sentence.replace('<e2>' + e2 + '</e2>', ' <e2> ' + e2 + ' </e2> ', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common  <e1> audits </e1>  were about  <e2> waste </e2>  and recycling.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1334,
     "status": "ok",
     "timestamp": 1586088550205,
     "user": {
      "displayName": "Rongfan Liao",
      "photoUrl": "",
      "userId": "07803922812103577726"
     },
     "user_tz": -480
    },
    "id": "Y0Gg4XIWTap8",
    "outputId": "505ca7b3-18af-49a0-8481-e954b9746a84"
   },
   "outputs": [],
   "source": [
    "class processor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    '''text cleansing'''\n",
    "    def search_entity(self,sentence):\n",
    "        # extract the entities\n",
    "        e1 = re.findall(r'<e1>(.*)</e1>', sentence)[0]\n",
    "        e2 = re.findall(r'<e2>(.*)</e2>', sentence)[0]\n",
    "        \n",
    "        sentence = sentence.replace('<e1>' + e1 + '</e1>', ' <e1> ' + e1 + ' </e1> ', 1)\n",
    "        sentence = sentence.replace('<e2>' + e2 + '</e2>', ' <e2> ' + e2 + ' </e2> ', 1)\n",
    "        \n",
    "        sentence = word_tokenize(sentence)\n",
    "        sentence = ' '.join(sentence)\n",
    "        sentence = sentence.replace('< e1 >', '<e1>')\n",
    "        sentence = sentence.replace('< e2 >', '<e2>')\n",
    "        sentence = sentence.replace('< /e1 >', '</e1>')\n",
    "        sentence = sentence.replace('< /e2 >', '</e2>')\n",
    "        sentence = sentence.split()\n",
    "\n",
    "        assert '<e1>' in sentence\n",
    "        assert '<e2>' in sentence\n",
    "        assert '</e1>' in sentence\n",
    "        assert '</e2>' in sentence\n",
    "        \n",
    "        ## two entiti location index finding\n",
    "        subj_start = subj_end = obj_start = obj_end = 0\n",
    "        \n",
    "        pure_sentence = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            if '<e1>' == word:\n",
    "                subj_start = len(pure_sentence)\n",
    "                continue\n",
    "            if '</e1>' == word:\n",
    "                subj_end = len(pure_sentence) - 1\n",
    "                continue\n",
    "            if '<e2>' == word:\n",
    "                obj_start = len(pure_sentence)\n",
    "                continue\n",
    "            if '</e2>' == word:\n",
    "                obj_end = len(pure_sentence) - 1\n",
    "                continue\n",
    "            pure_sentence.append(word)\n",
    "        return e1, e2, subj_start, subj_end, obj_start, obj_end, pure_sentence\n",
    "    \n",
    "    '''covert to be json format'''\n",
    "    \n",
    "    def convert(self,path_src, path_des):\n",
    "        with open(path_src, 'r', encoding='utf-8') as fr:\n",
    "            data = fr.readlines()\n",
    "        with open(path_des, 'w', encoding='utf-8') as fw:\n",
    "            for i in tqdm(range(0, len(data), 4)):\n",
    "                id_s, sentence = data[i].strip().split('\\t')\n",
    "                #每三行为一整个\n",
    "                sentence = sentence[1:-1]\n",
    "                e1, e2, subj_start, subj_end, obj_start, obj_end, sentence = self.search_entity(sentence)\n",
    "                meta = dict(\n",
    "                    id=id_s,\n",
    "                    relation=data[i+1].strip(),\n",
    "                    head=e1,\n",
    "                    tail=e2,\n",
    "                    subj_start=subj_start,\n",
    "                    subj_end=subj_end,\n",
    "                    obj_start=obj_start,\n",
    "                    obj_end=obj_end,\n",
    "                    sentence=sentence,\n",
    "                    comment=data[i+2].strip()[8:]\n",
    "                )\n",
    "                json.dump(meta, fw, ensure_ascii=False)\n",
    "                fw.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('audits', 'waste', 3, 3, 6, 6, ['The', 'most', 'common', 'audits', 'were', 'about', 'waste', 'and', 'recycling', '.'])\n"
     ]
    }
   ],
   "source": [
    "data_processor=processor()\n",
    "s=\"The most common <e1>audits</e1> were about <e2>waste</e2> and recycling.\"\n",
    "print(data_processor.search_entity(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:02<00:00, 2826.53it/s]\n"
     ]
    }
   ],
   "source": [
    "#train data\n",
    "data_processor.convert(\"./data/TRAIN_FILE.TXT\",'./data/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2717/2717 [00:01<00:00, 2697.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#train data\n",
    "data_processor.convert(\"./data/TEST_FILE_FULL.TXT\",'./data/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Load the pretrained word vectors\n",
    "### 1)\n",
    "input /输入：\n",
    "```\n",
    "word vectors file path /词向量文件地址\n",
    "```\n",
    "output /输出:\n",
    "```\n",
    "{'PAD': 0, 'sigarms': 1, 'katuna': 2, 'aqm': 3, '1.3775': 4, 'corythosaurus': 5, 'chanty': 6, 'kronik': 7, 'rolonda': 8, 'zsombor': 9, 'sandberger': 10, '*UNKNOWN*': 11}\n",
    "\n",
    "[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
    "        [ 2.6818e-01,  1.4346e-01, -2.7877e-01,  1.6257e-02,  1.1384e-01,\n",
    "          6.9923e-01, -5.1332e-01, -4.7368e-01, -3.3075e-01, -1.3834e-01,\n",
    "          2.7020e-01,  3.0938e-01, -4.5012e-01, -4.1270e-01, -9.9320e-02,\n",
    "          3.8085e-02,  2.9749e-02,  1.0076e-01, -2.5058e-01, -5.1818e-01,\n",
    "          3.4558e-01,  4.4922e-01,  4.8791e-01, -8.0866e-02, -1.0121e-01,\n",
    "         -1.3777e+00, -1.0866e-01, -2.3201e-01,  1.2839e-02, -4.6508e-01,\n",
    "          3.8463e+00,  3.1362e-01,  1.3643e-01, -5.2244e-01,  3.3020e-01,\n",
    "          3.3707e-01, -3.5601e-01,  3.2431e-01,  1.2041e-01,  3.5120e-01,\n",
    "         -6.9043e-02,  3.6885e-01,  2.5168e-01, -2.4517e-01,  2.5381e-01,\n",
    "          1.3670e-01, -3.1178e-01, -6.3210e-01, -2.5028e-01, -3.8097e-01],\n",
    "        [ 3.3042e-01,  2.4995e-01, -6.0874e-01,  1.0923e-01,  3.6372e-02,\n",
    "          1.5100e-01, -5.5083e-01, -7.4239e-02, -9.2307e-02, -3.2821e-01,\n",
    "          9.5980e-02, -8.2269e-01, -3.6717e-01, -6.7009e-01,  4.2909e-01,\n",
    "          1.6496e-02, -2.3573e-01,  1.2864e-01, -1.0953e+00,  4.3334e-01,\n",
    "          5.7067e-01, -1.0360e-01,  2.0422e-01,  7.8308e-02, -4.2795e-01,\n",
    "         -1.7984e+00, -2.7865e-01,  1.1954e-01, -1.2689e-01,  3.1744e-02,\n",
    "          3.8631e+00, -1.7786e-01, -8.2434e-02, -6.2698e-01,  2.6497e-01,\n",
    "         -5.7185e-02, -7.3521e-02,  4.6103e-01,  3.0862e-01,  1.2498e-01,\n",
    "         -4.8609e-01, -8.0272e-03,  3.1184e-02, -3.6576e-01, -4.2699e-01,\n",
    "          4.2164e-01, -1.1666e-01, -5.0703e-01, -2.7273e-02, -5.3285e-01],\n",
    "        [ 2.1705e-01,  4.6515e-01, -4.6757e-01,  1.0082e-01,  1.0135e+00,\n",
    "          7.4845e-01, -5.3104e-01, -2.6256e-01,  1.6812e-01,  1.3182e-01,\n",
    "         -2.4909e-01, -4.4185e-01, -2.1739e-01,  5.1004e-01,  1.3448e-01,\n",
    "         -4.3141e-01, -3.1230e-02,  2.0674e-01, -7.8138e-01, -2.0148e-01,\n",
    "         -9.7401e-02,  1.6088e-01, -6.1836e-01, -1.8504e-01, -1.2461e-01,\n",
    "         -2.2526e+00, -2.2321e-01,  5.0430e-01,  3.2257e-01,  1.5313e-01,\n",
    "          3.9636e+00, -7.1365e-01, -6.7012e-01,  2.8388e-01,  2.1738e-01,\n",
    "          1.4433e-01,  2.5926e-01,  2.3434e-01,  4.2740e-01, -4.4451e-01,\n",
    "          1.3813e-01,  3.6973e-01, -6.4289e-01,  2.4142e-02, -3.9315e-02,\n",
    "         -2.6037e-01,  1.2017e-01, -4.3782e-02,  4.1013e-01,  1.7960e-01],\n",
    "        [ 2.5769e-01,  4.5629e-01, -7.6974e-01, -3.7679e-01,  5.9272e-01,\n",
    "         -6.3527e-02,  2.0545e-01, -5.7385e-01, -2.9009e-01, -1.3662e-01,\n",
    "          3.2728e-01,  1.4719e+00, -7.3681e-01, -1.2036e-01,  7.1354e-01,\n",
    "         -4.6098e-01,  6.5248e-01,  4.8887e-01, -5.1558e-01,  3.9951e-02,\n",
    "         -3.4307e-01, -1.4087e-02,  8.6488e-01,  3.5460e-01,  7.9990e-01,\n",
    "         -1.4995e+00, -1.8153e+00,  4.1128e-01,  2.3921e-01, -4.3139e-01,\n",
    "          3.6623e+00, -7.9834e-01, -5.4538e-01,  1.6943e-01, -8.2017e-01,\n",
    "         -3.4610e-01,  6.9495e-01, -1.2256e+00, -1.7992e-01, -5.7474e-02,\n",
    "          3.0498e-02, -3.9543e-01, -3.8515e-01, -1.0002e+00,  8.7599e-02,\n",
    "         -3.1009e-01, -3.4677e-01, -3.1438e-01,  7.5004e-01,  9.7065e-01],\n",
    "        [ 2.3727e-01,  4.0478e-01, -2.0547e-01,  5.8805e-01,  6.5533e-01,\n",
    "          3.2867e-01, -8.1964e-01, -2.3236e-01,  2.7428e-01,  2.4265e-01,\n",
    "          5.4992e-02,  1.6296e-01, -1.2555e+00, -8.6437e-02,  4.4536e-01,\n",
    "          9.6561e-02, -1.6519e-01,  5.8378e-02, -3.8598e-01,  8.6977e-02,\n",
    "          3.3869e-03,  5.5095e-01, -7.7697e-01, -6.2096e-01,  9.2948e-02,\n",
    "         -2.5685e+00, -6.7739e-01,  1.0151e-01, -4.8643e-01, -5.7805e-02,\n",
    "          3.1859e+00, -1.7554e-02, -1.6138e-01,  5.5486e-02, -2.5885e-01,\n",
    "         -3.3938e-01, -1.9928e-01,  2.6049e-01,  1.0478e-01, -5.5934e-01,\n",
    "         -1.2342e-01,  6.5961e-01, -5.1802e-01, -8.2995e-01, -8.2739e-02,\n",
    "          2.8155e-01, -4.2300e-01, -2.7378e-01, -7.9010e-03, -3.0231e-02],\n",
    "        [ 4.4130e-01, -6.6569e-01,  5.3693e-01, -2.8451e-01, -4.9432e-01,\n",
    "          3.7877e-01,  2.9462e-01, -4.2007e-01, -3.3072e-01, -1.4959e-01,\n",
    "          1.7734e-02, -6.9029e-01, -4.1045e-01, -1.7590e-01,  3.5267e-01,\n",
    "          4.5984e-02, -5.5600e-01,  7.3799e-01,  1.3054e-01,  4.5681e-01,\n",
    "         -1.0072e-01,  9.9244e-01,  3.2525e-01, -4.8654e-01, -9.0020e-01,\n",
    "         -5.8867e-01,  3.7007e-01,  7.8460e-01,  2.5547e-01,  9.4015e-01,\n",
    "          5.3539e-01,  3.4617e-01,  1.6523e-01,  1.3020e-01, -6.2095e-01,\n",
    "         -1.2875e-01, -7.9159e-01, -7.6295e-01,  5.2197e-01, -7.5438e-02,\n",
    "         -3.8554e-01,  5.8315e-01, -7.1882e-02,  5.6333e-02, -2.8585e-01,\n",
    "         -3.5239e-01,  1.4035e-01,  8.2411e-01, -6.6001e-01,  1.9785e-01]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word embeeding\n",
    "class WordEmbeddingLoader(object):\n",
    "    \"\"\"\n",
    "    A loader for pre-trained word embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.path_word = config.embedding_path  # path of pre-trained word embedding\n",
    "        self.word_dim = config.word_dim  # dimension of word embedding\n",
    "    \n",
    "    def load_embedding(self):\n",
    "        word2id = dict()  # word to wordID\n",
    "        word_vec = list()  # wordID to word embedding\n",
    "\n",
    "        word2id['PAD'] = len(word2id)  # PAD character\n",
    "        #\n",
    "        with open(self.path_word, 'r', encoding='utf-8') as fr:\n",
    "            for line in fr:\n",
    "                line = line.strip().split()\n",
    "                if len(line) != self.word_dim + 1:\n",
    "                    continue\n",
    "                word2id[line[0]] = len(word2id)\n",
    "                word_vec.append(np.asarray(line[1:], dtype=np.float32))\n",
    "        if(\"*UNKNOWN*\" not in word2id):\n",
    "            word2id['*UNKNOWN*'] = len(word2id)\n",
    "            unk_emb= np.random.uniform(-1, 1,self.word_dim)\n",
    "            word_vec.append(unk_emb)\n",
    "        pad_emb = np.zeros([1, self.word_dim], dtype=np.float32)  # <pad> is initialize as zero\n",
    "        word_vec = np.concatenate((pad_emb, word_vec), axis=0)\n",
    "        word_vec = word_vec.astype(np.float32).reshape(-1, self.word_dim)\n",
    "        word_vec = torch.from_numpy(word_vec)\n",
    "        return word2id, word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "conf = namedtuple('conf',['embedding_path','word_dim'])\n",
    "config = conf(\"embedding/glove.6B.50d.txt\",50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf(embedding_path='embedding/glove.6B.50d.txt', word_dim=50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_loader=WordEmbeddingLoader(config)\n",
    "word2id, word_vec=emd_loader.load_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Relation Encoding\n",
    "input /输入：\n",
    "```\n",
    "relation encoding file path /关系编码文件地址\n",
    "```\n",
    "output /输出:\n",
    "```\n",
    "rel2id:\n",
    "\n",
    "{'Other': 0, 'Cause-Effect(e1,e2)': 1, 'Cause-Effect(e2,e1)': 2, 'Component-Whole(e1,e2)': 3, 'Component-Whole(e2,e1)': 4, 'Content-Container(e1,e2)': 5, 'Content-Container(e2,e1)': 6, 'Entity-Destination(e1,e2)': 7, 'Entity-Destination(e2,e1)': 8, 'Entity-Origin(e1,e2)': 9, 'Entity-Origin(e2,e1)': 10, 'Instrument-Agency(e1,e2)': 11, 'Instrument-Agency(e2,e1)': 12, 'Member-Collection(e1,e2)': 13, 'Member-Collection(e2,e1)': 14, 'Message-Topic(e1,e2)': 15, 'Message-Topic(e2,e1)': 16, 'Product-Producer(e1,e2)': 17, 'Product-Producer(e2,e1)': 18}\n",
    "\n",
    "id2rel:\n",
    "\n",
    "{0: 'Other', 1: 'Cause-Effect(e1,e2)', 2: 'Cause-Effect(e2,e1)', 3: 'Component-Whole(e1,e2)', 4: 'Component-Whole(e2,e1)', 5: 'Content-Container(e1,e2)', 6: 'Content-Container(e2,e1)', 7: 'Entity-Destination(e1,e2)', 8: 'Entity-Destination(e2,e1)', 9: 'Entity-Origin(e1,e2)', 10: 'Entity-Origin(e2,e1)', 11: 'Instrument-Agency(e1,e2)', 12: 'Instrument-Agency(e2,e1)', 13: 'Member-Collection(e1,e2)', 14: 'Member-Collection(e2,e1)', 15: 'Message-Topic(e1,e2)', 16: 'Message-Topic(e2,e1)', 17: 'Product-Producer(e1,e2)', 18: 'Product-Producer(e2,e1)'}\n",
    "\n",
    "class_num:\n",
    "19\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 694,
     "status": "ok",
     "timestamp": 1586088525904,
     "user": {
      "displayName": "Rongfan Liao",
      "photoUrl": "",
      "userId": "07803922812103577726"
     },
     "user_tz": -480
    },
    "id": "KiD7fbjc0Ie4",
    "outputId": "9f8a8808-3fbe-4e03-a062-dd41e1d678f6"
   },
   "outputs": [],
   "source": [
    "class RelationLoader(object):\n",
    "    def __init__(self, config):\n",
    "        self.data_dir = config.data_dir\n",
    "\n",
    "    def __load_relation(self):\n",
    "        relation_file = os.path.join(self.data_dir, 'relation2id.txt')\n",
    "        rel2id = {}\n",
    "        id2rel = {}\n",
    "        with open(relation_file, 'r', encoding='utf-8') as fr:\n",
    "            for line in fr:\n",
    "                relation, id_s = line.strip().split()\n",
    "                id_d = int(id_s)\n",
    "                rel2id[relation] = id_d\n",
    "                id2rel[id_d] = relation\n",
    "        return rel2id, id2rel, len(rel2id)\n",
    "\n",
    "    def get_relation(self):\n",
    "        return self.__load_relation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! head data/relation2id.txt\n",
    "from collections import namedtuple\n",
    "conf = namedtuple('conf',['data_dir'])\n",
    "config = conf(\"data\")\n",
    "rel_loader=RelationLoader(config)\n",
    "rel2id, id2rel, class_num=rel_loader.get_relation()\n",
    "# print(rel_loader.get_relation()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1200,
     "status": "ok",
     "timestamp": 1586088529191,
     "user": {
      "displayName": "Rongfan Liao",
      "photoUrl": "",
      "userId": "07803922812103577726"
     },
     "user_tz": -480
    },
    "id": "VBhP0eY71EIC",
    "outputId": "ac4ec01f-c2de-4695-de9d-b31c8d2779d4"
   },
   "source": [
    "# 4 training sample generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 defind the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1059,
     "status": "ok",
     "timestamp": 1586088533224,
     "user": {
      "displayName": "Rongfan Liao",
      "photoUrl": "",
      "userId": "07803922812103577726"
     },
     "user_tz": -480
    },
    "id": "lTV7BzGC1wNz",
    "outputId": "703f9ce6-ce64-46e8-ef50-3e18501e825e"
   },
   "outputs": [],
   "source": [
    "class SemEvalDateset(Dataset):\n",
    "    def __init__(self, filename, rel2id, word2id, config):\n",
    "        self.filename = filename\n",
    "        self.rel2id = rel2id\n",
    "        self.word2id = word2id\n",
    "        self.max_len = config.max_len\n",
    "        self.pos_dis = config.pos_dis\n",
    "        self.data_dir = config.data_dir\n",
    "        self.dataset, self.label = self.__load_data()\n",
    "        \n",
    "    # position encoding\n",
    "    # pos_dis: parameter for position encoding: the length to shift the position\n",
    "    def __get_pos_index(self, x):\n",
    "        if x < -self.pos_dis:\n",
    "            return 0\n",
    "        if x >= -self.pos_dis and x <= self.pos_dis:\n",
    "            return x + self.pos_dis + 1\n",
    "        if x > self.pos_dis:\n",
    "            return 2 * self.pos_dis + 2\n",
    "        \n",
    "    # relative position encoding\n",
    "    def __get_relative_pos(self, x, entity_pos):\n",
    "        # entity_pos[0] -> begin\n",
    "        # entity_pos[1] -> end\n",
    "        if x < entity_pos[0]:\n",
    "            return self.__get_pos_index(x-entity_pos[0])\n",
    "        elif x > entity_pos[1]:\n",
    "            return self.__get_pos_index(x-entity_pos[1])\n",
    "        else:\n",
    "            return self.__get_pos_index(0)\n",
    "        \n",
    "    #sentence feature\n",
    "    def _symbolize_sentence(self, e1_pos, e2_pos, sentence):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                e1_pos (tuple) span of e1\n",
    "                e2_pos (tuple) span of e2\n",
    "                sentence (list)\n",
    "        \"\"\"\n",
    "        \n",
    "        mask = [1] * len(sentence)\n",
    "        # for exmaple  [1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3]\n",
    "        # 1 for the positions before the first entity\n",
    "        # 2 for the positions bwt first ane second entity\n",
    "        # 3 for the positions after the second entity\n",
    "        \n",
    "        if e1_pos[0] < e2_pos[0]:\n",
    "            for i in range(e1_pos[0], e2_pos[1]+1):\n",
    "                mask[i] = 2\n",
    "            for i in range(e2_pos[1]+1, len(sentence)):\n",
    "                mask[i] = 3\n",
    "        else:\n",
    "            for i in range(e2_pos[0], e1_pos[1]+1):\n",
    "                mask[i] = 2\n",
    "            for i in range(e1_pos[1]+1, len(sentence)):\n",
    "                mask[i] = 3\n",
    "\n",
    "        words = [] # words id list\n",
    "        pos1 = [] # word position relative to es1\n",
    "        pos2 = [] # word pisition relative to es2\n",
    "        \n",
    "        length = min(self.max_len, len(sentence))\n",
    "        mask = mask[:length]\n",
    "\n",
    "        for i in range(length):\n",
    "            words.append(self.word2id.get(sentence[i], self.word2id['*UNKNOWN*']))\n",
    "            pos1.append(self.__get_relative_pos(i, e1_pos))\n",
    "            pos2.append(self.__get_relative_pos(i, e2_pos))\n",
    "        \n",
    "        # PADDING\n",
    "        if length < self.max_len:\n",
    "            for i in range(length, self.max_len):\n",
    "                mask.append(0)  # 'PAD' mask is zero\n",
    "                words.append(self.word2id['PAD'])\n",
    "\n",
    "                pos1.append(self.__get_relative_pos(i, e1_pos))\n",
    "                pos2.append(self.__get_relative_pos(i, e2_pos))\n",
    "        unit = np.asarray([words, pos1, pos2, mask], dtype=np.int64)\n",
    "        unit = np.reshape(unit, newshape=(1, 4, self.max_len))\n",
    "        return unit\n",
    "    \n",
    "    # lexical feature\n",
    "    def _lexical_feature(self,e1_idx,e2_idx, sent):\n",
    "        \n",
    "        def _entity_context(e_idx,sent):\n",
    "            ''' return [w(e-1), w(e), w(e+1)]\n",
    "            '''\n",
    "            context = []\n",
    "            context.append(sent[e_idx])\n",
    "            if e_idx >= 1:\n",
    "                context.append(sent[e_idx-1])\n",
    "            else:\n",
    "                context.append(sent[e_idx])\n",
    "\n",
    "            if e_idx < len(sent)-1:\n",
    "                context.append(sent[e_idx+1])\n",
    "            else:\n",
    "                context.append(sent[e_idx])\n",
    "            return context\n",
    "        \n",
    "        # to find the right and left word\n",
    "        context1 = _entity_context(e1_idx[0], sent)\n",
    "        context2 = _entity_context(e2_idx[0], sent)\n",
    "        \n",
    "        # ignore WordNet hypernyms in paper\n",
    "        lexical = context1 + context2\n",
    "#         print(sent)\n",
    "#         print(lexical)\n",
    "        lexical_ids=[self.word2id.get(word, self.word2id['*UNKNOWN*']) for word in lexical]\n",
    "        lexical_ids=np.asarray(lexical_ids, dtype=np.int64)\n",
    "#         print(lexical_ids)\n",
    "        return np.reshape(lexical_ids, newshape=(1, 6))\n",
    "    \n",
    "    def __load_data(self):\n",
    "        path_data_file = os.path.join(self.data_dir, self.filename)\n",
    "        data = []\n",
    "        labels = []\n",
    "        with open(path_data_file, 'r', encoding='utf-8') as fr:\n",
    "            for line in fr:\n",
    "                line = json.loads(line.strip())\n",
    "                label = line['relation']\n",
    "                sentence = line['sentence']\n",
    "                e1_pos = (line['subj_start'], line['subj_end'])\n",
    "                e2_pos = (line['obj_start'], line['obj_end'])\n",
    "                label_idx = self.rel2id[label]\n",
    "\n",
    "                one_sentence = self._symbolize_sentence(e1_pos, e2_pos, sentence)\n",
    "                \n",
    "                lexical = self._lexical_feature(e1_pos, e2_pos, sentence)\n",
    "                \n",
    "                temp = (one_sentence, lexical)\n",
    "                data.append(temp)\n",
    "                #data.append(one_sentence)\n",
    "                labels.append(label_idx)\n",
    "        return data, labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        label = self.label[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! head data/relation2id.txt\n",
    "from collections import namedtuple\n",
    "conf = namedtuple('conf',['max_len','pos_dis','data_dir'])\n",
    "config = conf(100,50,\"data\")\n",
    "filename='train.json'\n",
    "data_loader=SemEvalDateset(filename, rel2id, word2id,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19894,   862,    36,  3632,    60,     6]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example for lexical features encoded\n",
    "e1_idx = (3,3)\n",
    "e2_idx = (6,6)\n",
    "sent=['The', 'most', 'common', 'audits', 'were', 'about', 'waste', 'and', 'recycling', '.']\n",
    "data_loader._lexical_feature(e1_idx,e2_idx,sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for sentence with position features encoding\n",
    "e1_idx = (3,3)\n",
    "e2_idx = (6,6)\n",
    "sent=['The', 'most', 'common', 'audits', 'were', 'about', 'waste', 'and', 'recycling', '.']\n",
    "sentence_features = data_loader._symbolize_sentence(e1_idx,e2_idx,sent)\n",
    "# return  [words id, pos enc relative to  entity 1, pos enc relative to entity 2, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[400001,     97,    862,  19894,     36,     60,   3632,      6,\n",
       "          12521,      3,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0],\n",
       "        [    48,     49,     50,     51,     52,     53,     54,     55,\n",
       "             56,     57,     58,     59,     60,     61,     62,     63,\n",
       "             64,     65,     66,     67,     68,     69,     70,     71,\n",
       "             72,     73,     74,     75,     76,     77,     78,     79,\n",
       "             80,     81,     82,     83,     84,     85,     86,     87,\n",
       "             88,     89,     90,     91,     92,     93,     94,     95,\n",
       "             96,     97,     98,     99,    100,    101,    102,    102,\n",
       "            102,    102,    102,    102,    102,    102,    102,    102,\n",
       "            102,    102,    102,    102,    102,    102,    102,    102,\n",
       "            102,    102,    102,    102,    102,    102,    102,    102,\n",
       "            102,    102,    102,    102,    102,    102,    102,    102,\n",
       "            102,    102,    102,    102,    102,    102,    102,    102,\n",
       "            102,    102,    102,    102],\n",
       "        [    45,     46,     47,     48,     49,     50,     51,     52,\n",
       "             53,     54,     55,     56,     57,     58,     59,     60,\n",
       "             61,     62,     63,     64,     65,     66,     67,     68,\n",
       "             69,     70,     71,     72,     73,     74,     75,     76,\n",
       "             77,     78,     79,     80,     81,     82,     83,     84,\n",
       "             85,     86,     87,     88,     89,     90,     91,     92,\n",
       "             93,     94,     95,     96,     97,     98,     99,    100,\n",
       "            101,    102,    102,    102,    102,    102,    102,    102,\n",
       "            102,    102,    102,    102,    102,    102,    102,    102,\n",
       "            102,    102,    102,    102,    102,    102,    102,    102,\n",
       "            102,    102,    102,    102,    102,    102,    102,    102,\n",
       "            102,    102,    102,    102,    102,    102,    102,    102,\n",
       "            102,    102,    102,    102],\n",
       "        [     1,      1,      1,      2,      2,      2,      2,      3,\n",
       "              3,      3,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0]]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_features # [words id, pos enc for entity 1, pos enc for entity 2, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader output\n",
    "data,label = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[400001,    279,     20,    980,   1070,     32,     48,   2606,\n",
       "            3251,      7,     30,  40634,  11465,      4,  13874,   2623,\n",
       "               3,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0],\n",
       "         [    39,     40,     41,     42,     43,     44,     45,     46,\n",
       "              47,     48,     49,     50,     51,     52,     53,     54,\n",
       "              55,     56,     57,     58,     59,     60,     61,     62,\n",
       "              63,     64,     65,     66,     67,     68,     69,     70,\n",
       "              71,     72,     73,     74,     75,     76,     77,     78,\n",
       "              79,     80,     81,     82,     83,     84,     85,     86,\n",
       "              87,     88,     89,     90,     91,     92,     93,     94,\n",
       "              95,     96,     97,     98,     99,    100,    101,    102,\n",
       "             102,    102,    102,    102,    102,    102,    102,    102,\n",
       "             102,    102,    102,    102,    102,    102,    102,    102,\n",
       "             102,    102,    102,    102,    102,    102,    102,    102,\n",
       "             102,    102,    102,    102,    102,    102,    102,    102,\n",
       "             102,    102,    102,    102],\n",
       "         [    36,     37,     38,     39,     40,     41,     42,     43,\n",
       "              44,     45,     46,     47,     48,     49,     50,     51,\n",
       "              52,     53,     54,     55,     56,     57,     58,     59,\n",
       "              60,     61,     62,     63,     64,     65,     66,     67,\n",
       "              68,     69,     70,     71,     72,     73,     74,     75,\n",
       "              76,     77,     78,     79,     80,     81,     82,     83,\n",
       "              84,     85,     86,     87,     88,     89,     90,     91,\n",
       "              92,     93,     94,     95,     96,     97,     98,     99,\n",
       "             100,    101,    102,    102,    102,    102,    102,    102,\n",
       "             102,    102,    102,    102,    102,    102,    102,    102,\n",
       "             102,    102,    102,    102,    102,    102,    102,    102,\n",
       "             102,    102,    102,    102,    102,    102,    102,    102,\n",
       "             102,    102,    102,    102],\n",
       "         [     1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      2,      2,      2,      2,\n",
       "               3,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0]]], dtype=int64),\n",
       " array([[11465, 40634,     4,  2623, 13874,     3]], dtype=int64))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data # ([words id, pos enc relative to  entity 1, pos enc relative to entity 2, mask],lexical feature)\n",
    "# lexical feature = (left of entity1 ,entity1,right of entity 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Defind Data Loader\n",
    "Define loader.__collate_fn convert data to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义loader\n",
    "class SemEvalDataLoader(object):\n",
    "    def __init__(self, rel2id, word2id, config):\n",
    "        self.rel2id = rel2id\n",
    "        self.word2id = word2id\n",
    "        self.config = config\n",
    "\n",
    "    def __collate_fn(self, batch):\n",
    "        data, label = zip(*batch)  # unzip the batch data\n",
    "        data = list(data)\n",
    "        label = list(label)\n",
    "        # cover array to be tensor\n",
    "        sentence_feat = torch.from_numpy(np.concatenate([x[0] for x in data], axis=0)) # word id\n",
    "        lexical_feat = torch.from_numpy(np.concatenate([x[1] for x in data], axis=0)) # pos for entity_1\n",
    "        label = torch.from_numpy(np.asarray(label, dtype=np.int64))\n",
    "        return (sentence_feat,lexical_feat),label\n",
    "\n",
    "    def __get_data(self, filename, shuffle=False):\n",
    "        dataset = SemEvalDateset(filename, self.rel2id, self.word2id, self.config)\n",
    "        loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=2,\n",
    "            collate_fn=self.__collate_fn\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def get_train(self):\n",
    "        return self.__get_data('train.json', shuffle=True)\n",
    "\n",
    "    def get_dev(self):\n",
    "        return self.__get_data('test.json', shuffle=False)\n",
    "\n",
    "    def get_test(self):\n",
    "        return self.__get_data('test.json', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
